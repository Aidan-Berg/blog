[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I am Aidan Berg (he/him), an epidemiologist working as a research data specialist at the University of Illinois School of Social Work. Currently, much of my work focuses on independent evaluation of Illinois Medicaid 1115 Behavioral Health Transformation Demonstration Waiver. I have an affinity for data cleaning, manipulation, analysis, and visualization with a variety of software packages, primarily R. When I am not working on data analysis, I am usually writing: grant reports, policy evaluation, IRB requests, PHI data applications, and whatever other technical writing can move our projects at UIUC’s Center for Prevention Research and Development forward.\nWhen I’m not at work, I enjoy biking, board games, and reading (mostly horror novels by Shirley Jackson and Peter Straub).\n\n\n\nUniversity of Illinois at Urbana-Champaign | Urbana, IL\nMPH in Epidemiology | August 2021 - December 2022\nUniversity of Illinois at Urbana-Champaign | Urbana, IL\nB.S. in Community Health | August 2018 - May 2021\n\n\n\nCenter for Prevention Research and Development | Research Data Specialist | January 2023 - Present\nUniversity of Illinois Extension | Student Researcher | February 2019 - December 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! I am Aidan Berg (he/him), an epidemiologist working as a research data specialist at the University of Illinois School of Social Work. Currently, much of my work focuses on independent evaluation of Illinois Medicaid 1115 Behavioral Health Transformation Demonstration Waiver. I have an affinity for data cleaning, manipulation, analysis, and visualization with a variety of software packages, primarily R. When I am not working on data analysis, I am usually writing: grant reports, policy evaluation, IRB requests, PHI data applications, and whatever other technical writing can move our projects at UIUC’s Center for Prevention Research and Development forward.\nWhen I’m not at work, I enjoy biking, board games, and reading (mostly horror novels by Shirley Jackson and Peter Straub)."
  },
  {
    "objectID": "posts/Tidy-Tuesday-10-24-23/index.html",
    "href": "posts/Tidy-Tuesday-10-24-23/index.html",
    "title": "#TidyTuesday, 10/24/23",
    "section": "",
    "text": "After a long while of trying to explain exactly what I do to curious family and friends, I have decided to start up a data science blog, to replace my old professional website, and to maybe develop a bit of a portfolio of my work. Since I am a dedicated useR, and have always wanted to be involved in the R community, I decided to jump into #TidyTuesday as my first post! I won’t waste any more time and import the dataset.\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tt_load(2023, week = 43)\n\n\n    Downloading file 1 of 1: `patient_risk_profiles.csv`\npatient_risk_profiles &lt;- tuesdata$patient_risk_profiles\n\nhead(patient_risk_profiles)\n\n# A tibble: 6 × 100\n  personId `age group:  10 -  14` `age group:  15 -  19` `age group:  20 -  24`\n     &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1        1                      0                      0                      0\n2        2                      0                      0                      0\n3        3                      0                      0                      0\n4        4                      0                      0                      0\n5        5                      0                      0                      0\n6        6                      0                      0                      0\n# ℹ 96 more variables: `age group:  65 -  69` &lt;dbl&gt;,\n#   `age group:  40 -  44` &lt;dbl&gt;, `age group:  45 -  49` &lt;dbl&gt;,\n#   `age group:  55 -  59` &lt;dbl&gt;, `age group:  85 -  89` &lt;dbl&gt;,\n#   `age group:  75 -  79` &lt;dbl&gt;, `age group:   5 -   9` &lt;dbl&gt;,\n#   `age group:  25 -  29` &lt;dbl&gt;, `age group:   0 -   4` &lt;dbl&gt;,\n#   `age group:  70 -  74` &lt;dbl&gt;, `age group:  50 -  54` &lt;dbl&gt;,\n#   `age group:  60 -  64` &lt;dbl&gt;, `age group:  35 -  39` &lt;dbl&gt;, …\nLuckily for me, it seems that I’ve gotten the best dataset an epidemiologist could ask for – health data! More specifically, it is “100 simulated patient’s medical history features and the predicted 1-year risk of 14 outcomes based on each patient’s medical history features”, per the tidytuesday repo\nIt also seems that the R/Pharma Conference opted to send in a slightly messy dataset. This shouldn’t be problem, I tend to enjoy the data cleaning process1, it’s like a form of spreadsheet kintsugi, where I can make a broken dataset more beautiful and informative. Looking to the actual dataset, it seems we have 100 variables, which have some unconventional names. Additionally, age group is spread out over 19 columns, which are slightly out of order. I’ll start by trimming the age group columns into one ordered factor variable using case_when() from dplyr and some of the forcats functions, then I’ll chip away at the column names.\nlibrary(tidyverse)\n\npatient_risk_profiles &lt;- patient_risk_profiles %&gt;%\n  mutate(age_group = case_when(\n    `age group:   0 -   4` == 1 ~ \"0-4\",\n    `age group:   5 -   9` == 1 ~ \"5-9\",\n    `age group:  10 -  14` == 1 ~ \"10-14\",\n    `age group:  15 -  19` == 1 ~ \"15-19\",\n    `age group:  20 -  24` == 1 ~ \"20-24\",\n    `age group:  25 -  29` == 1 ~ \"25-29\",\n    `age group:  30 -  34` == 1 ~ \"30-34\",\n    `age group:  35 -  39` == 1 ~ \"35-39\",\n    `age group:  40 -  44` == 1 ~ \"40-44\",\n    `age group:  45 -  49` == 1 ~ \"45-49\",\n    `age group:  50 -  54` == 1 ~ \"50-54\",\n    `age group:  55 -  59` == 1 ~ \"55-59\",\n    `age group:  60 -  64` == 1 ~ \"60-64\",\n    `age group:  65 -  69` == 1 ~ \"65-69\",\n    `age group:  70 -  74` == 1 ~ \"70-74\",\n    `age group:  75 -  79` == 1 ~ \"75-79\",\n    `age group:  80 -  84` == 1 ~ \"80-84\",\n    `age group:  85 -  89` == 1 ~ \"85-89\",\n    `age group:  90 -  94` == 1 ~ \"90-94\"\n  ))\n  \npatient_risk_profiles$age_group &lt;- patient_risk_profiles$age_group %&gt;% \n  as_factor() # now convert to factor\n\nlevels(patient_risk_profiles$age_group) # check levels\n\n [1] \"5-9\"   \"80-84\" \"45-49\" \"30-34\" \"60-64\" \"75-79\" \"15-19\" \"65-69\" \"55-59\"\n[10] \"35-39\" \"10-14\" \"20-24\" \"0-4\"   \"40-44\" \"25-29\" \"70-74\" \"90-94\" \"50-54\"\n[19] \"85-89\"\n\npatient_risk_profiles$age_group &lt;- patient_risk_profiles$age_group %&gt;% \n  fct_relevel(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", # manually fix\n              \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\",\n              \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80-84\", \"85-89\",\n              \"90-94\")\n\nlevels(patient_risk_profiles$age_group) # fixed!\n\n [1] \"0-4\"   \"5-9\"   \"10-14\" \"15-19\" \"20-24\" \"25-29\" \"30-34\" \"35-39\" \"40-44\"\n[10] \"45-49\" \"50-54\" \"55-59\" \"60-64\" \"65-69\" \"70-74\" \"75-79\" \"80-84\" \"85-89\"\n[19] \"90-94\"\n\n# cut down on column bloat\npatient_risk_profiles &lt;- patient_risk_profiles %&gt;%\n  select(!c(`age group:  10 -  14`:`age group:  90 -  94`))\nOkay, things are already looking better! It was a bit tedious to reorder the age groups, but there are 18 fewer columns to deal with. Next, I’m just going to drop the Sex = MALE column – most datasets I’ve seen just use one column for sex assigned at birth. Then, I am going to cut down the variable names to shorthand, before tossing them into janitor to force all column names to snake case. I’m also removing the word predicted from the predicted risk variables. This is because the word ‘risk’, to me, already implies a prediction. The same goes for the string “Occurrence of”. Also, I just now noticed that the variables which should be simple 0-1 factors are in fact coded as integers. I will fix this as well.\npatient_risk_profiles &lt;- patient_risk_profiles %&gt;% \n  select(!`Sex = MALE`) %&gt;%\n  rename(sex = `Sex = FEMALE`)\n\n# while we're here, sex should really be a factor\npatient_risk_profiles$sex &lt;- patient_risk_profiles$sex %&gt;%\n  factor(levels = c(0, 1),\n         labels = c(\"Male\", \"Female\"))\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# names to shorthand\npatient_risk_profiles &lt;- patient_risk_profiles %&gt;% \n  rename_with(~str_replace(.,\" in \",\"_\")) %&gt;%\n  rename_with(~str_remove(., \" at \")) %&gt;%\n  rename_with(~str_remove(., \"predicted \")) %&gt;%\n  rename_with(~str_remove(., \"Occurrence of \")) %&gt;%\n  clean_names()\n\n# I'm sure someone is wincing because I couldn't figure out how to use one \n# rename_with command. This is entirely fair.\n  \npatient_risk_profiles &lt;- patient_risk_profiles %&gt;%\n  mutate(\n    across(\n      acetaminophen_exposures_prior_year:antibiotics_tetracyclines_prior_year,\n      as.factor\n  ))\n\nhead(patient_risk_profiles)\n\n# A tibble: 6 × 81\n  person_id sex   acetaminophen_exposu…¹ alcoholism_prior_year anemia_prior_year\n      &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;                  &lt;fct&gt;                 &lt;fct&gt;            \n1         1 Fema… 1                      0                     0                \n2         2 Male  0                      0                     0                \n3         3 Male  0                      0                     0                \n4         4 Male  0                      0                     0                \n5         5 Fema… 0                      0                     1                \n6         6 Male  0                      0                     0                \n# ℹ abbreviated name: ¹​acetaminophen_exposures_prior_year\n# ℹ 76 more variables: angina_events_prior_year &lt;fct&gt;,\n#   antiepileptics_prior_year &lt;fct&gt;, anxiety_prior_year &lt;fct&gt;,\n#   osteoarthritis_prior_year &lt;fct&gt;, aspirin_exposures_prior_year &lt;fct&gt;,\n#   asthma_prior_year &lt;fct&gt;, atrial_fibrillation_incident_prior_year &lt;fct&gt;,\n#   hormonal_contraceptives_prior_year &lt;fct&gt;,\n#   any_cancer_excl_prostate_cancer_and_benign_cancer_prior_year &lt;fct&gt;, …\nThese names are still not perfect2. However, most of the common patterns of messy name have been stamped out, and future fixes will occur on a case-by-case basis. I also made sure to retain the prior_year portion of each column name where it occured, because without it, it may be difficult to figure out which variable is which.\nYes, this is a bit lazy, but with 81 columns to manually fix, I think I’ll turn to analysis. This dataset so far has given me a warm sense of nostalgia for my biostatistics classes in graduate school, even down to a logistic regression output! Back then I was using STATA as my analysis software of choice3, which was fun, but never had the suite of data visualization tools that drew me into R in the first place. Furthermore, I have pivoted my research to explore substance use disorder, and there are a few variables which I can use to explore some association. We have a variable for past-year alcoholism, opioids and smoking4, all of which I research on a daily basis! I think it could be interesting to see how well past-year substance use5 correlates with risk of a few of the health conditions given in the dataset, controlling for age, since we spent so long creating an age variable. For this, I’ll need to coalesce substance use into one variable (mostly to lump together cases where a patient had multiple instances of substance abuse), then create my visualizations.\npatient_risk_profiles &lt;- patient_risk_profiles %&gt;%\n  mutate(substance_use_prior_year = case_when(\n    opioids_prior_year == 0 & \n      alcoholism_prior_year == 0 & \n      smoking_prior_year == 0 \n    ~ \"No substance use\",\n    opioids_prior_year == 1 ~ \"Opioids\",\n    alcoholism_prior_year == 1 ~ \"Alcoholism\",\n    smoking_prior_year == 1 ~ \"Smoking\",\n    .default = \"Multiple substance use\" \n                 # lump patients with multiple instances of prior year\n  ))             # use into 'other'\n  \npatient_risk_profiles$substance_use_prior_year &lt;- \n  patient_risk_profiles$substance_use_prior_year %&gt;%\n  as_factor()\n\nggplot(patient_risk_profiles) +\n  geom_jitter(aes(x = age_group,\n                  y = risk_of_dementia,\n                  color = age_group)) +\n  labs(x = \"Age Group\",\n       y = \"Dementia Risk\") +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5)) +\n  facet_wrap(vars(substance_use_prior_year)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5),\n        legend.key = element_blank(),\n        panel.background = element_blank(),\n        panel.grid = element_blank(),\n        panel.grid.major.y = element_line(colour = \"gray\", linewidth = .25),\n        axis.line = element_line(color = \"black\"))\nThese charts do not seem to tell us much, except that it seems that age is (seemingly) exponentially associated with dementia risk; while certainly interesting, it is not an unusual correlation to find. Furthermore, it seems that the sample sizes are too small to make any meaningful conjectures about any subpopulation besides the population which did not have prior year smoking, alcoholism, or opioid use. Let’s try another angle then – maybe a different chart will help. Let’s take a look at depression risk, since mental health and substance use are frequently intertwined. I will also take the log of the risk to see things more clearly.\nggplot(patient_risk_profiles) +\n  geom_jitter(aes(x = age_group,\n                  y = log(risk_of_treatment_resistant_depression_trd),\n                  color = substance_use_prior_year)) +\n  labs(x = \"Age Cohort\",\n       y = \"Log of Risk of Treatment Resistant\\n Depression\") +\n  guides(color = guide_legend(title = \"Prior Year Substance Use\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5),\n        legend.key = element_blank(),\n        panel.background = element_blank(),\n        panel.grid = element_blank(),\n        panel.grid.major.y = element_line(colour = \"gray\", size = .25),\n        axis.line = element_line(color = \"black\"))\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nAnother swing, another messy scatter plot. There are some prior year opioid users who have elevated risk of TRD in the range of 20-24, but these three don’t quite represent an association. I’ll try one more time, with migraine risk.\nggplot(patient_risk_profiles) +\n  geom_jitter(aes(x = age_group,\n                  y = log(risk_of_migraine),\n                  color = substance_use_prior_year)) +\n  labs(x = \"Age Cohort\",\n       y = \"Log of Risk of Treatment\\n Resistant Depression\") +\n  guides(color = guide_legend(title = \"Prior Year Substance Use\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5),\n        legend.key = element_blank(),\n        panel.background = element_blank(),\n        panel.grid = element_blank(),\n        panel.grid.major.y = element_line(colour = \"gray\", size = .25),\n        axis.line = element_line(color = \"black\"))\nAnd another messy scatterplot6. I think I will call it here. While I could easily spend hours working backwards to derive the models used in the dataset to generate the risk profiles for each simulated patient, setting up logistic regressions of my own, calculating risk rations7 I do already spend much of my day coding with data that is extremely similar to this, so I’ll spare myself some work-outside-of-work. Still, I had quite a bit of fun exploring some of the forcats functions and working with the column strings – time is never wasted learning how to clean up a dataset. I’ve also been exploring the more powerful aspects of ggplot2, as you can see with the charts edited to look a bit less like default ggplot output.\nI will try to participate in #TidyTuesday every few weeks or so; it’s nice to use RStudio outside of a work setting, and I feel like the practice is invaluable. See you around!\nThe album I chose to scrub this dataset to was Disco Volador, by the Orielles."
  },
  {
    "objectID": "posts/Tidy-Tuesday-11-07-23/index.html",
    "href": "posts/Tidy-Tuesday-11-07-23/index.html",
    "title": "#TidyTuesday, 11/7/23",
    "section": "",
    "text": "I am back for another #TidyTuesday! After the somewhat disappointing results I found in my last post, I was relieved to find that this week’s dataset is the US House Election Results from 1976-2022, as presented by the MIT Election Data and Science Lab (Data and Lab 2017).\nWhile I am more of a health researcher, I’m always happy to put on my 538-hat1 and try my hand at political science. While the amount of coverage of elections in the US is certainly tiresome, it does give statisticians a lot of data to play around with, which, as a statistician who lives outside of a battleground state, is quite nice.\nMy non-battleground state is Illinois, which is famously a stronghold of the Democratic Party: as of writing, the last time the state voted for a Republican candidate was 35 years ago, in 1988! Since then, Illinois has only become more and more liberal. So why, then, am I exploring a state which has been, at least in presidential election terms, unexciting since Mikail Gorbachev led the Soviet Union?"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "Hello! I am Aidan Berg (he/him), an epidemiologist working as a research data specialist at the University of Illinois School of Social Work. Currently, much of my work focuses on independent evaluation of Illinois Medicaid 1115 Behavioral Health Transformation Demonstration Waiver. I have an affinity for data cleaning, manipulation, analysis, and visualization with a variety of software packages, primarily R. When I am not working on data analysis, I am usually writing: grant reports, policy evaluation, IRB requests, PHI data applications, and whatever other technical writing can move our projects at UIUC’s Center for Prevention Research and Development forward.\nWhen I’m not at work, I enjoy biking, board games, and reading (mostly horror novels by Shirley Jackson and Peter Straub)."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "University of Illinois at Urbana-Champaign | Urbana, IL\nMPH in Epidemiology | August 2021 - December 2022\nUniversity of Illinois at Urbana-Champaign | Urbana, IL\nB.S. in Community Health | August 2018 - May 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "Center for Prevention Research and Development | Research Data Specialist | January 2023 - Present\nUniversity of Illinois Extension | Student Researcher | February 2019 - December 2022"
  },
  {
    "objectID": "posts/Tidy-Tuesday-11-07-23/index.html#downstate-upstate-collar-counties-and-the-like",
    "href": "posts/Tidy-Tuesday-11-07-23/index.html#downstate-upstate-collar-counties-and-the-like",
    "title": "#TidyTuesday, 11/7/23",
    "section": "Downstate, upstate, collar counties, and the like",
    "text": "Downstate, upstate, collar counties, and the like\nBecause, if you get under the hood of the seedy underbelly2 of Illinois politics, we find a very interesting situation. Illinois’ largest urban area is Chicago in Cook County, and Chicago is a large city3. Chicago is the state’s largest city, and it’s suburbs contain the 2nd, 3rd, 4th, and 6th largest cities by population. This has led to a lopsided population imbalance between the city of Chicago and it’s closest, oldest suburbs, the newer suburban counties surrounding Chicago, and the rest of the state.\nAll this is common knowledge, so to quickly explain everything: Chicago and Cook County are Chicago, the collar counties are the 5 counties which adjoin Cook County4, and ‘downstate’ is the remaining 96 counties in Illinois. Personally, I’m always astonished by the sheer size of Chicago, where so many people live that the majority of the state is simple ‘downstate’. This has led to the political dynamics that I’d like to explore today.\nYou see, the congressional districts in Chicago have almost always been democratic strongholds, those ‘downstate’ have been republican strongholds. The collar counties, however, containing those election-swaying suburban voters, tend to flip-flop. The votes of the collar counties are the reason why Republican candidates clung to hope in state elections as Illinois liberalized, such as Former Governor Bruce Rauner’s term from 2015 to 2019."
  },
  {
    "objectID": "posts/Tidy-Tuesday-11-07-23/index.html#footnotes",
    "href": "posts/Tidy-Tuesday-11-07-23/index.html#footnotes",
    "title": "#TidyTuesday, 11/7/23",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs someone who gained political awareness, and then political agency, in the heated 2016 and 2018 elections, Nate Silver was a huge inspiration for a budding data scientist.↩︎\nWhile this might be a potshot at Illinois’ famously corrupt politics, I’m going to cut my native state a bit of slack (for now).↩︎\ncitation needed↩︎\nSpecifically, DuPage, Kane, Lake, McHenry, and Will County.↩︎\nCalifornia comes close, but the urban/rural delineation is different due to the presence of multiple large urban areas (e.g., Los Angeles, San Francisco, San Diego, San Jose, etc. etc.), while New York and Illinois have one very, very large city.↩︎\nGo Illini!↩︎"
  },
  {
    "objectID": "posts/Tidy-Tuesday-11-07-23/index.html#picking-through-the-data",
    "href": "posts/Tidy-Tuesday-11-07-23/index.html#picking-through-the-data",
    "title": "#TidyTuesday, 11/7/23",
    "section": "Picking through the data",
    "text": "Picking through the data\nAnyways, these intra-state dynamics, seemingly defined by an urban/rural divide between Chicagoland and downstate Illinois, are what make political studies within Illinois so interesting, since the only other state with a similar makeup is New York5. Without furtherado, let’s crack open the dataset. I have a very specific idea of the visualization I’d like to make, inspired by this dumbbell plot from the R Graph Gallery. I’d also like to make some maps, since I have been wanting to work more with R’s mapping/GIS capabilities. I know for a fact that a map of just Illinois will be insufficient, since the congressional districts of Chicago are quite famous for their gerrymandered complexity, so I’ll have to make an inset map. That’s two visualizations, so I’ll make those and call it a day.\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)    # for just about everything\nlibrary(patchwork)    # for stitching charts together\nlibrary(tidytuesdayR) # for loading the data\nlibrary(sf)           # for making maps\n\ntuesdata &lt;- tt_load('2023-11-07')\n\n\n    Downloading file 1 of 1: `house.csv`\n\nhouse &lt;- tuesdata$house\n\nmap &lt;- read_sf(\"cb_2022_us_cd118_500k.shp\")\n\nGreat, the easy part is complete! Now to narrow the data to just Illinois’ 2022 election results for the 118th Congress.\n\nmap &lt;- map %&gt;%\n  filter(STATEFP == 17) %&gt;% \n  rename(\"district\" = \"CD118FP\") %&gt;%             # We'll match on this later\n  mutate(district = str_remove(district, \"^0+\")) # remove all leading zeros\n\n# filter, reduce third party and write-in candidates into one category\n# this is the cleaning pipeline\nhouse_il &lt;- house %&gt;%\n  filter(state_po == \"IL\" & year == 2022) %&gt;% # narrow the dataset\n  select(-c(candidate, \n            unofficial, \n            version,                          # drop unneeded columns\n            fusion_ticket, \n            state_po, \n            state_fips)) %&gt;%\n  mutate(district = str_remove(district, \"^0+\"), # remove leading zero\n         district = as_factor(district),         # district needs to be factor\n         party = replace_na(party, \"third_party\"),\n         party = as_factor(party),\n         party = fct_other(party,                # clean up third/write-in votes\n                           keep = c(\"REPUBLICAN\", \"DEMOCRAT\"),\n                           other_level = \"Third Party\"),\n         percent = round((candidatevotes / totalvotes) * 100,\n                         digits = 2)) %&gt;%\n  group_by(district, party) %&gt;%\n  reframe(percent = sum(percent)) # collapse into three categories per district\n\n# make it wide, compute additional variables for plotting\nhouse_il_wide &lt;- house_il %&gt;%\n  pivot_wider(names_from = party, values_from = percent) %&gt;%\n  rename(\"Democrat\" = \"DEMOCRAT\",        # clean up names, at last\n         \"Republican\" = \"REPUBLICAN\") %&gt;%\n  mutate(gap = round(Democrat - Republican, digits = 0), # compute gap\n         chicagoland = as_factor( # flag all districts in Chicago metro area\n           if_else(district %in% c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14),\n                   \"Chicagoland\", \"Downstate IL\"))) %&gt;%\n  group_by(district) %&gt;%\n  mutate(max = max(Democrat, Republican), # who had the best percent of the vote\n         min = min(Democrat, Republican)) # this'll be useful for the plots\n\n# pivot back to long for easier plots, retain original for backup         \nhouse_il_long &lt;- house_il_wide %&gt;%\n  pivot_longer(c(Democrat, Republican, `Third Party`),\n               names_to = \"party\",\n               values_to = \"percent\")\n\nWow, those were some very long pipes! However, it was mostly cleaning and flip-flopping the data from long to wide to long again to compute the little bits and bobs that I’ll call on for the chart. I also added a variable for whether or not a district is in the Chicago metropolitan area, chicagoland, since this is the relationship I’d like to explore. It was also my first time using the reframe() function, and I must say I am a fan. Now, let’s make the dumbbell plot. My goal is to show the percent of the vote captured by each party, the margin of victory for the winner, and whether or not the district was in Chicagoland. Here’s what I came up with.\n\ndumbbellplot &lt;- \n  ggplot(house_il_long, aes(x = percent, y = district)) +\n  geom_segment(aes(x = min,\n                   xend = max,\n                   y = district,\n                   yend = district),\n               color = \"#E7E7E7\",\n               linewidth = 3.5) +\n  geom_point(aes(color = party), size = 3) + \n  geom_text(aes(label = percent, color = party),\n            size = 3.25,\n            nudge_x = case_when(\n              house_il_long$party == \"Third Party\" ~ 1,\n              house_il_long$percent == house_il_long$max &\n                house_il_long$party != \"Third Party\" ~ 1,\n              house_il_long$percent != house_il_long$max &\n                house_il_long$party != \"Third Party\" ~ -1,\n              house_il_long$percent &gt; 95 ~ -4), # this is nasty, but functional\n            hjust = case_when(\n              house_il_long$party == \"Third Party\" ~ 0,\n              house_il_long$percent == house_il_long$max &\n                house_il_long$party != \"Third Party\" ~ 0,\n              house_il_long$percent != house_il_long$max &\n                house_il_long$party != \"Third Party\" ~ 1)) +\n  theme_minimal() +\n  theme(legend.title = element_blank(),\n        legend.direction = \"vertical\",\n        legend.position = c(.90, .5), # legend within chart for detail\n        axis.text.y = element_text(color=\"black\"),\n        axis.text.x = element_text(color=\"#989898\"),\n        axis.title = element_blank(),\n        panel.grid = element_blank(),\n        strip.text = element_blank()) +\n  labs(x = \"%\", y = NULL) +\n  scale_color_manual(values = c(\"navy\", \"darkred\", \"#111111\")) +\n  scale_x_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_y_discrete(limits = rev) +\n  facet_grid(chicagoland ~ .,\n             scales = \"free_y\")\n\n # this is just the dumbbell plot, I'll need another chart to add the margin\n\n# this will make the chart look a hair more detailed (e.g., +5D over 5)\nhouse_il_wide &lt;- house_il_wide %&gt;%\n  mutate(gap_party_max = case_when(Democrat == max ~ \"D\",\n                                   Republican == max ~ \"R\"),\n         gap_label = if_else(\n           is.na(gap), \"No contest\", paste0(\"+\", abs(gap), gap_party_max)) %&gt;%\n           fct_inorder())\n\ngapcol &lt;- ggplot(house_il_wide, aes(gap, district)) +\n  geom_text(aes(0, label = gap_label, color = gap_party_max),\n            fontface = \"bold\",\n            size = 3.25) +\n  facet_grid(chicagoland ~ .,\n             scales = \"free_y\") +\n  scale_color_manual(values = c(\"navy\", \"darkred\", \"#111111\")) +\n  scale_y_discrete(limits = rev) +\n  theme_void() +\n  theme(plot.margin = margin(l = 0, r=0, b=0, t=0),\n        panel.background = element_rect(fill=\"#EFEFE3\", color=\"#EFEFE3\"),\n        legend.position = \"none\",\n        strip.text = element_text(angle = -90, \n                                  vjust = .6))\nwhole_chart &lt;- \n  dumbbellplot + gapcol +\n  plot_layout(design = c(area(l=0,  r=45, t=0, b=1),\n                         area(l=46, r=52, t=0, b=1))) +\n  plot_annotation(title = \"Margin of victory by congressional district\",\n                  subtitle = \"Chicagoland districts compared to 'downstate' Illinois\",\n                  caption = \"Source: MIT Election Data and Science Lab\")\nwhole_chart\n\n\n\n\nAgain, after a lot of painstaking code, and much work under the hood tweaking alignment and spacing, we have a plot that looks very good. Already, the dynamics are fascinating. The Democratic party is not only strong in Chicagoland, but dominates it. In the 7th district, the Republican party didn’t even put up a candidate! Downstate, things are different. the 12th, 15th, and 16th districts are very heavily leaned Republican, although the 13th is strongly Democratic, probably due to the presence of college-town Champaign6 and East St. Louis. In fact, the only district close to a battleground is the 17th, possibly because of the large swathes of rural areas and the Quad Cities setting up the district as the closest thing to a competition in the whole state.\nI am quite pleased with this chart, but let’s make some maps to show another angle of the Chicagoland/downstate dynamics. This will allow us to see the impact of the collar-counties a bit better, and it will show how gerrymandering is still an issue in Illinois.\n\nmapping_data &lt;- full_join(map, house_il_wide, by = \"district\") # join map data \n\nstate_map &lt;- ggplot(mapping_data) +\n  geom_sf(aes(fill = if_else(is.na(gap), 99.4, gap))) +\n  scale_fill_gradient2(low = \"darkred\", \n                       high = \"navy\", \n                       midpoint = 0, \n                       name = \"Margin of Victory,\\nPercentage Points\") +\n  theme_void()\n\nchi_map &lt;- ggplot(mapping_data) +\n  geom_sf(aes(fill = if_else(is.na(gap), 99.4, gap))) +\n  coord_sf(xlim = c(-88.55, -87.5),\n           ylim = c(41.5, 42.5),\n           expand = FALSE) +\n  scale_fill_gradient2(low = \"darkred\", \n                       high = \"navy\", \n                       midpoint = 0, \n                       name = \"Margin of Victory,\\nPercentage Points\") +\n  theme_void()\n\nmargin_map &lt;- state_map + \n  chi_map + \n  plot_layout(guides = 'collect') +\n  plot_annotation(title = \"Margin of victory by congressional district\",\n                  subtitle = \"Illinois, with Chicago detail\",\n                  caption = \"Source: MIT Election Data and Science Lab\")\n\nmargin_map\n\n\n\n\nPerfect! We can almost see a gradient from the rural districts to the loop of Chicago. This suggests an association which I’m sure University of Chicago poly-sci undergradutes understand more deeply than I can imagine. We can also see the effect of gerrymandering, to some extent (why does the 13th, the pale blue district in the south, stretch 3/4 of the way across the state?) Illinois has been cleaning up it’s districts, thanks to a steady decline in population leading to redistricting, but it’s clear that there’s some work to be done.\nEither way, that’s me for today. I could make all of these charts much, much better, like adding labels to the districts on the map, centering the Chicago inset instead of using the border with Wisconsin as an anchor and generally cleaning up fonts and spacing, but the hour is late, and I’d like to wrap up. This was a lot of fun, the amount of detail than can be sunk into each chart is nearly infinite, and I was happy to get some more experience working with maps. Till next time!\nThe album I listened to while picking apart the politics of Illinois was Illinois by Surfjan Stevens\n\nReferences\n\n\nData, MIT Election, and Science Lab. 2017. “U.S. House 1976–2022.” Harvard Dataverse. https://doi.org/10.7910/DVN/IG0UN2."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "TidyTuesday, 11/7/23\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nVisualising election results in Illinois\n\n\n\n\n\n\nNov 7, 2023\n\n\nAidan Berg\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday, 10/24/23\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\nAnother data scientist enters the fray\n\n\n\n\n\n\nOct 24, 2023\n\n\nAidan Berg\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "About Me",
    "section": "",
    "text": "Hello! I am Aidan Berg (he/him), an epidemiologist working as a research data specialist at the University of Illinois School of Social Work. Currently, much of my work focuses on independent evaluation of Illinois Medicaid 1115 Behavioral Health Transformation Demonstration Waiver. I have an affinity for data cleaning, manipulation, analysis, and visualization with a variety of software packages, primarily R. When I am not working on data analysis, I am usually writing: grant reports, policy evaluation, IRB requests, PHI data applications, and whatever other technical writing can move our projects at UIUC’s Center for Prevention Research and Development forward.\nWhen I’m not at work, I enjoy biking, board games, and reading (mostly horror novels by Shirley Jackson and Peter Straub)."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nUniversity of Illinois at Urbana-Champaign | Urbana, IL\nMPH in Epidemiology | August 2021 - December 2022\nUniversity of Illinois at Urbana-Champaign | Urbana, IL\nB.S. in Community Health | August 2018 - May 2021"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nCenter for Prevention Research and Development | Research Data Specialist | January 2023 - Present\nUniversity of Illinois Extension | Student Researcher | February 2019 - December 2022"
  },
  {
    "objectID": "index.html#awards-and-accolades",
    "href": "index.html#awards-and-accolades",
    "title": "About Me",
    "section": "Awards and Accolades",
    "text": "Awards and Accolades\n\nOutstanding Graduate Capstone Award | Henderson County Infectious Disease Plan | University of Illinois at Urbana-Champaign | December 2022\nHighest Honors | University of Illinois at Urbana-Champaign, 2022\nDean’s List | University of Illinois at Urbana-Champaign\n\nFall 2018\nSpring 2018\nFall 2019\nSpring 2019\nFall 2020\nSpring 2021"
  },
  {
    "objectID": "posts/Tidy-Tuesday-10-24-23/index.html#footnotes",
    "href": "posts/Tidy-Tuesday-10-24-23/index.html#footnotes",
    "title": "#TidyTuesday, 10/24/23",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThank goodness↩︎\nIn particular, both columns I called in the last mutate function are especially horrid, but I don’t want to take away the antibiotics string because otherwise it’s a useful landmark when navigating the dataset, similar to risk and prior_year.↩︎\nAnd by my choice, I mean per the syllabus.↩︎\nAnd steroids, however, I am more familiar with the effects of those I’ve mentioned, so I’ll leave steroids be for now.↩︎\nAlcoholism will have to serve as a proxy for heavy drinking – this isn’t quite accurate, but for now I’m just chumming the waters.↩︎\nAlthough the parabola formed with risk of depression raising during adulthood and dropping in childhood and old age is quite interesting…↩︎\nfinding AIC and BIC for each model, plotting logistic regression outputs, deriving risk ratios, comparing my models to the risk profiles provided…↩︎"
  }
]